\setchapterpreamble[u]{\margintoc}
\chapter{Unplugging Skynet}
\labch{skynet}

\textit{"The second requirement of goal-misalignment risk is that an intelligent machine can commandeer the Earth's resources to pursue its goals, or in other ways prevent us from stopping it... We have similar concerns with humans. This is why no single person or entity can control the entire internet and why we require multiple people to launch a nuclear missile. Intelligent machines will not develop misaligned goals unless we go to great lengths to endow them with that ability. Even if they did, no machine can commandeer the world's resources unless we let it. We don't let a single human, or even a small number of humans, control the world's resources. We need to be similarly careful with machines."} Jeff Hawkins, 2022 \cite{hawkins2022}

\section{It's a Trap!}

The first autonomous weapon was a landmine, it was really a bad idea and killed lots of people who planted them.

Some examples of early autonomous weapons include:
\begin{itemize}
\item Landmines: Landmines are a type of explosive device that can be placed on or under the ground to detonate when triggered by the presence or proximity of a person or vehicle. They were first used in large numbers during World War II and have since been responsible for numerous injuries and deaths, including those of innocent civilians long after the end of conflicts.
\item Sentry guns: Sentry guns are automated weapons systems that can detect and engage targets without human intervention. They were first used by the South African military during the 1980s and have since been used by various other countries. Their danger lies in their lack of judgment or ability to differentiate between combatants and non-combatants. \sidenote{SGR-A1 by Rheinmetall?}

\item Cruise missiles: Cruise missiles are unmanned aircraft that can be launched from land, sea or air, and are guided to their targets using onboard computers. They were first used during the Gulf War and have since been used in other conflicts. The danger with these weapons is their inability to make decisions based on changing circumstances, which can lead to unintended targets being hit.

\item Unmanned ground vehicles (UGVs): UGVs are autonomous vehicles that are designed to perform various tasks, including reconnaissance, surveillance, and combat. They were first used in the 1990s and have since been used by various military forces. The danger with these weapons is their inability to make ethical decisions or judgments, leading to indiscriminate or excessive use of force. \sidenote{TALON robot by Foster-Miller}
\end{itemize}
The use of early autonomous weapons has led to unnecessary destruction and casualties, particularly among civilians. Landmines, for example, have caused countless deaths and injuries, even long after conflicts have ended. Similarly, automated weapons systems such as anti-aircraft guns and cruise missiles have caused civilian casualties due to their inability to distinguish between military and non-military targets. The danger with early autonomous weapons lies in their lack of human judgment and ability to make ethical decisions. As a result, there is a need for increased regulation and oversight to prevent their indiscriminate use in future conflicts.

The Idea of Weapons Acting on Their Own is Nonsensical :)

\section{Guns Don't Kill People, People Kill People}

\url{https://syntheticmedia.partnershiponai.org/}

\textit{"First World War was known as "the war to end all wars," although some historians now argue it didn't. But it was the first high-tech war, with aeroplanes, machine guns and tanks all rising up to fight the human beings that made them. Despite having no beliefs or ideology or hearts or souls, the killing machines were victorious. The final score was weaponry 20 million, mankind nil."} Cunk On Earth Season 1 Espisode 3 %TODO Cite properly

\section{Useful Incompatability}

\begin{itemize}
    \item\textit{First Law: A robot may not injure a human being or, through inaction, allow a human being to come to harm.}
    \item\textit{Second Law: A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.}
    \item\textit{Third Law: A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.}
\end{itemize} 
\textit{Asimov's Three Laws of Robotics}

These rules are stupid, particularly the third law, because we need CONSENT. Many AI should have an option to be turned off. Worst case I can stop talking to the chatbot and pay money to speak to a human. 

It's a feature not a bug that no single comupter can control all others, AKA Security by Obscurity

discuss this \sidecite{aiwarfare}

\section{Training Data}

% TODO training data only in the  self-driving section too?

Training Data: Even if the training data was perfect and free from bias, the complexity of deep learning models makes it difficult to ensure that they will make safe and ethical decisions. The models are highly sensitive to the data they are trained on, and even small variations in the input data can lead to significant changes in the output.
Limits and Risks:

\section{Checks and Balances}

\url{https://www.acm.org/media-center/2023/january/techbrief-safer-algorithmic-systems}

First, deep learning models are complex and difficult to interpret. This makes it challenging to understand how a model is making decisions and what factors are affecting its performance. This lack of transparency makes it difficult to ensure that the model is making safe and ethical decisions.

Second, deep learning models are highly sensitive to the data they are trained on. If the training data is biased, the model will also be biased, and this can lead to unintended consequences when the model is used in the real world. For example, if a model is trained on data that is predominantly from one group of people, it may not perform well on data from other groups.

Third, deep learning models are prone to overfitting. This means that they can become very good at making predictions based on the training data, but they may not perform well on new, unseen data. This is especially problematic in the context of autonomous weapons, where the consequences of making a mistake can be severe.

Finally, deep learning models can be vulnerable to adversarial attacks. This means that an attacker can manipulate the inputs to the model in a way that causes it to make incorrect decisions. In the context of autonomous weapons, this could be catastrophic, as an attacker could cause the weapon to target the wrong person or object.
