\setchapterpreamble[u]{\margintoc}
\chapter{Unplugging Skynet}
\labch{skynet}

\textit{"The second requirement of goal-misalignment risk is that an intelligent machine can commandeer the Earth's resources to pursue its goals, or in other ways prevent us from stopping it... We have similar concerns with humans. This is why no single person or entity can control the entire internet and why we require multiple people to launch a nuclear missile. Intelligent machines will not develop misaligned goals unless we go to great lengths to endow them with that ability. Even if they did, no machine can commandeer the world's resources unless we let it. We don't let a single human, or even a small number of humans, control the world's resources. We need to be similarly careful with machines."} Jeff Hawkins, 2022 \cite{hawkins2022}

\section{AI and Human Safety}

\textit{"First World War was known as "the war to end all wars," although some historians now argue it didn't. But it was the first high-tech war, with aeroplanes, machine guns and tanks all rising up to fight the human beings that made them. Despite having no beliefs or ideology or hearts or souls, the killing machines were victorious. The final score was weaponry 20 million, mankind nil."} Cunk On Earth Season 1 Espisode 3 %TODO Cite properly

\section{Useful Incompatability}

\begin{itemize}
    \item\textit{First Law: A robot may not injure a human being or, through inaction, allow a human being to come to harm.}
    \item\textit{Second Law: A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.}
    \item\textit{Third Law: A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.}
\end{itemize} 
\textit{Asimov's Three Laws of Robotics}

These rules are stupid, particularly the third law, because we need CONSENT. Many AI should have an option to be turned off. Worst case I can stop talking to the chatbot and pay money to speak to a human. 

It's a feature not a bug that no single comupter can control all others, AKA Security by Obscurity

\section{Training Data}

% TODO training data only in the  self-driving section too?

Training Data: Even if the training data was perfect and free from bias, the complexity of deep learning models makes it difficult to ensure that they will make safe and ethical decisions. The models are highly sensitive to the data they are trained on, and even small variations in the input data can lead to significant changes in the output.
Limits and Risks:

\section{Checks and Balances}

\url{https://www.acm.org/media-center/2023/january/techbrief-safer-algorithmic-systems}

First, deep learning models are complex and difficult to interpret. This makes it challenging to understand how a model is making decisions and what factors are affecting its performance. This lack of transparency makes it difficult to ensure that the model is making safe and ethical decisions.

Second, deep learning models are highly sensitive to the data they are trained on. If the training data is biased, the model will also be biased, and this can lead to unintended consequences when the model is used in the real world. For example, if a model is trained on data that is predominantly from one group of people, it may not perform well on data from other groups.

Third, deep learning models are prone to overfitting. This means that they can become very good at making predictions based on the training data, but they may not perform well on new, unseen data. This is especially problematic in the context of autonomous weapons, where the consequences of making a mistake can be severe.

Finally, deep learning models can be vulnerable to adversarial attacks. This means that an attacker can manipulate the inputs to the model in a way that causes it to make incorrect decisions. In the context of autonomous weapons, this could be catastrophic, as an attacker could cause the weapon to target the wrong person or object.
