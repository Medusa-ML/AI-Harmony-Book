\setchapterpreamble[u]{\margintoc}
\chapter{Self-Driving With Statistics}
\labch{driving}

\begin{itemize}
    \item\textit{First Law: A robot may not injure a human being or, through inaction, allow a human being to come to harm.}
    \item\textit{Second Law: A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.}
    \item\textit{Third Law: A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.}
\end{itemize} 
\textit{Asimov's Three Laws of Robotics}

\section{Self-Driving Horses}

\section{Semiautonomy is Stupid}

Talk about the levels 0-4, deep learning is a subsystem of these levels.

Semiautonomy is dumb because it feels autonomous, but the user is expected to take over at any time. This is the worst of both worlds. \cite{torchinsky_boeckmann_2019}

and this article \href{https://www.theautopian.com/newly-released-video-of-thanksgiving-day-tesla-full-self-driving-crash-demonstrates-the-fundamental-problem-of-semi-automated-driving-systems/}{Tesla Crash Illustrates Problem With Semi-Automated Driving} and \href{https://news.ycombinator.com/item?id=34347778}{comments}

\section{Trolley Problems}

\section{Concept Drift (Reprise)}

\section{Multicolinearity (Reprise)}

\sidenote{You can't get your self-driving car dirty either, it'll mess up those sensors.}

\section{Explaining The Unexplainable in Court}

\textit{"Limitations and Pitfalls of Explainable and Interpretable Methods}

\textit{Before diving into the exact methods for interpreting and explaining models, let's take a look at some of the pitfalls of these methods.}

\textit{First off, if you need to make high-stakes decisions, make sure to use inherently interpretable models\sidenote{This sentence is very important, they are saying DO NOT USE DEEP LEARNING TECHNIQUES FOR CRITICAL DECISION MAKING. This is wild, we are already using these models everywhere, it's like asking people not to use computers.... TODO EXPAND ON THIS.}. These are models such as decision trees that are more readily converted to output explanations.}

\textit{Before choosing a method, you need to be absolutely clear about what you want out of it. Are you trying to understand the nature of the data procurement process? How a decision was made? How the model works on a fundamental level? Some tools might be appropriate for some of these goals but not others.}

\textit{If your goal is to make sense of the data generation process, this is only possible if you know that your model already generalizes well to unseen data.}

\textit{Decision interpretability can be misleading. It highlights things like correlations, but doesnâ€™t go into the level of causal detail that causal inference does. Remember that correlation does not (always) imply causation.}

\textit{WARNING}
\textit{Spurious correlations can result in inaccurate interpretations even with advanced interpretability methods like saliency methods and attention-based methods.}

\textit{Tools such as feature importance usually estimate mean values, but one should beware the error bars on those means and take stock of the confidence intervals.}

\textit{A lot of machine learning involves working with extremely high-dimensional spaces. There's no way around it: high-dimensional data and feature spaces are hard to make sense of without grouping the data or features together first.}

\textit{Even if you do find important features in those matrices, remember that this does not imply causality (we've said this before and we'll say it again)."} \cite{trustworthyml}

\section{A Train is a Self-Driving Car, Right?}

Discuss how the problem space has changed in warehousing, we don't actually have self-driving forklifts, we have moving shelves.

Maybe talk about elon musk and manufacturing.
