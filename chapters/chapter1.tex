\setchapterpreamble[u]{\margintoc}
\chapter{The Artificial Intelligence Industrial Complex}
\labch{introduction}

\textit{""} Marc Andreesen, 2023 \cite{welsh2023}

\section{The AI Bomb}

“Car Crash Climax” that causes readers to say, “how did he do that?!?” but don’t reveal it all at once… piece it out over the entire manuscript. Give the reader at least 65 of the manuscript where you describe HOW you did it (i.e. solved the problem, helped a company avoid bankruptcy, evolve, etc.)

\section{Quants, The Flash Crash and Rennaissance Technologies} 

\section{Dr. Frankenstein at Microsoft, Google and Facebook}

\section{Scientists, Sharing and Stealing}

\section{Human and AI Workers of The World, Unite!}

AI is outsourcing, too. a continuation of a long trend.

If you can't beat them, join them.

\section{Old Intro}

Artificial Intelligence (AI) is not magic. It's here, and it's changing the world. Deep learning is one of the most exciting and popular fields of AI, but it's not the same as the good old-fashioned rules-based AI of the past. Deep learning involves training models by repeatedly showing them large datasets and allowing the models to infer the rules between input and output data.

Deep learning models are trained on data, almost like humans. However, the quality of the data is critical to the functioning of the model. For stable and well-understood environments like chess, chemistry or Newtonian physics, we can collect and generate data and deep learning can do a tremendous amount of useful work for us. In less stable environments where the rules of the day sometimes do not reflect the rules of the past, deep learning can be less helpful and even cause real harm when naively deployed. 

I'll discuss dozens of models in detail but consider any data collected that involves complex social interactions. Start with family interactions, then romantic ones and then consider that topics like advertising, trading stocks, credit scoring and even hate speech and threat detection all might have a dynamic social component to them. Models predictive power will suffer if the past does not look like the future. This is a problem for deep learning models that are trained on old data.

Also consider that as a creator of deep learning models, I can use a model to editorialize. I can train a model on data that fits my worldview instead of data that fits the world as it is. As a user or investor, how would you stop me? My stock trading model would be the first one to get me in trouble. If it was regularly monitored and managed, my model could do about as much harm as a troublesome employee could. 

But what about my model that is used to provide online dating advice, credit scores, or acceptance to students to elite universities? It might take a few years before my editorializing was found out, depending on how well it was managed and how egregious my model's outputs are.

Some of these problems are small potatoes, who cares if online dating sites don't do good science to suggest matches? What about full-self driving though? The realization of autonomous cars we're told is perpetually years away, but is it really possible with our current roads, laws and infrastructure? What happens if a model stops being updated, and it becomes the height of fashion for kids to wear shirts with street-signs on them? Do all of the cars stop working? Also, we're calling this autopilot, but don't pilots in planes need to file flight plans, speak to other planes and take instructions from a tower? Do our cars need to do that too? 

If you understand the type of AI that is being used (rules-based or deep learning), the data it was trained on, and monitor the AI in production you are on the road to success. But, what if I told you I could use the outputs from the model you just spent a billion dollars building and very easily create a new model that performs almost as well, and spend almost nothing building it. Furthermore, if you took me to court I could show my work and prove that I made the model myself. Would this change the way you invest in artificial intelligence? Would it change the way you develop and share your models?

Back to self-driving cars, Here is how that could pan out. Imagine Honda comes from behind and creates the worlds greatest fully self-driving car and it cost them only \$1 billion to make. Now imagine I buy a couple of those Hondas, maybe 100 of them for \$30,000 each, then pay 100 drivers \$100,000 per year to drive them, and equip each honda with \$70,000 of my own computers and sensors. At the end of this endeavour I could theoretically create a decent machine learning model using the "Honda's" data for 20\% of their investment. There are of course other costs I would incur, but if you come along with me and assume that the data is the most important component of the model, then I have all of the data I need on the cheap.

I'll disucss this in detail in chapter 5, but for now let me tell you I'm skeptical of full self-driving without a lot of infrastructure changes, but I'm not skeptical of the ability to create a model that can drive a car. I'm skeptical of the ability to create a model that can drive a car in a way that is safe, reliable and that will hold up in court.   

The cost of innovating is well known, and this is why patent protection exists. But deep learning models are trained on data that is free or easy to copy and in a way that produces slightly different and seemingly random inner-workings on each training run, if this is the case then I don't see many defensible positions for innovators in machine learning. Let's consider another hypothetical from pharmacology, imagine Pfizer invented the cancer curing pill, then I put that pill in a machine that came up with a significantly different formulation that achieved the same results, and when you gave it to the expert witness chemists they would have to say "the chemistry of these two pills is fundamentally different, but they both cure cancer", which would make it very hard to defend the original cancer pill's patent in court.

Because of the way they are trained, deep learning models introduce real mathematical chaos wherever they are deployed. This leads me to a few conclusions that I'll give you here in the introduction, but explain in detail in the meat of this book: 

\begin{enumerate}
\item Deep learning models can make statistically informed decisions based on the data they were previously shown, but because of their size they can produce seemingly random results. Poor data collection (or editorializing) leads to poor results. Anything using deep learning models cannot be used by itself to make critical decisions. Said differently, there must be supervision and outside control wherever deep learning is involved.
\item Any decision involving deep learning models is functionally unexplainable, and therefore likely to get someone in trouble in court. Any domain where deep learning is used to make a decision and then asked to explain in detail how that decision was made should be greeted with a shrug from the witness stand.
\item Deep learning intellectual property is indefensible, it is built in a way that is both easily copyable, and impossible to verify that it was actually copied.
\end{enumerate}

Deep learning introduces challenges for some, but opportunities for others. I am a member of the \href{https://www.fsf.org/}{Free Software Foundation}, and from my perspective deep learning models are one type of software that might inherently support the foundation's purpose. The purpose of the foundation is to promote the universal freedom to distribute and modify computer software without restriction. If deep learning models are simultaneously powerful and free, they become rocket fuel for innovation. 

This is the silver lining to the "myths" that I'd like to discuss in this book. Deep learning is messy, data science is hard, but as a tool deep learning is absolutely mindblowing. I can rank order my emails by their sarcasm, create avatars of my friends in the style of Disney characters, have ChatGPT summarize the DaVinci Code for my book report, and have my deep learning model suggest possible life saving drugs for me. The world is fantastic and will get better thanks to this tool, but like any tool we should use it safely and appropriately.  

Consider utilizing deep learning as an "employee" for any non-critical tasks that you don't wish to perform yourself. I personally fired my virtual assistant from Brickwork India (\$200/month) and hired ChatGPT (\$20 per month). By managing it, you can put yourself in an excellent position. In my opinion, the world won't end up in a dystopian \href{https://en.wikipedia.org/wiki/The_Terminator}{\textit{Terminator}}-esque state, nor will work disappear in a utopian \href{https://en.wikipedia.org/wiki/Fully_Automated_Luxury_Communism}{\textit{Fully-Automated Luxury Communism}} scenario. Instead, we'll find ourselves somewhere in the middle, and it'll likely be more gratifying. Work will transform, we will supervise and manage our new technological workers, and they'll be cheap! This new management job won't require us to give up our agency but to act as masters of a new realm where our attention and thought are required, and where vital decisions are still made by us.


\section{Key Takeaways}

\begin{itemize}
	\item \textbf{AI is a misleading term}, we should instead talk about rules-based programming (GOFAI) and deep learning so we don't confuse ourselves, our partners and our users.\sidenote{Avoid the imprecise words "Algorithms" and "AI" and instead consider the more precise "rules-based AI" AKA "GOFAI" to describe situations where programmers explicitly write rules that they design and "deep learning" or "machine learning" where programmers create models using a dataset and let the machine figure out the rules via a neural network.} 
	\item Good Old-Fashioned AI \textbf{(rules-based AI) is hard to create and maintain}. We used to use it for chess engines (like Deep Blue) and translation, but now programmers favor using statistical machine learning techniques.
	\item Good Old-Fashioned AI is not well suited to many problems, like machine translation and handwriting detection.
	\item Modern Deep Learning techniques use data to train models instead of humans explicitly writing rules, and the \textbf{deep learning models are often as good as the data they are trained with}.\sidenote{More \textit{intelligent} data and maybe less intelligent programmers, or maybe programmers who are better trained in statistics and complex systems theory instead of "classical" computer science.}
\end{itemize}

\begin{marginfigure}[-5.5cm]
        \includegraphics{bored_programmer}
        \caption{"a frustrated programmer writing boring rules on his computer" made with Stable Diffusion 2.1}
        \labfig{marginprogrammer}
\end{marginfigure}
