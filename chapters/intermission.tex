\setchapterpreamble[u]{\margintoc}
\chapter{Intermission: Example Models}
\labch{intro}

\section{Lightning Round of Model Analysis}

\begin{enumerate}
    \item \textbf{A text generation model trained on Shakespeare's plays} This could be used to spruce up dialoge for a screenplay, write a book, or write a fancy-sounding email. It should be obvious to the user what it is doing, and this would probably delight users and not cause many problems.
    \item \textbf{A classifer to suggest promising lithium mining sites}
    \item \textbf{A generative model to discover drugs that might be useful in preventing liver disease}
    \item \textbf{A in image classifier for a dating app, trained on images of famous couples} Someone wants to start their own dating app, but they don't have any of their own data. So they base their "compatability score" off of photos of potential couple and classify them as either "looking like a celebrity couple, and therefore compatable" or "not looking like a celebrity couple, and therefore incompatable". This model sounds funny and interesting, but could certainly change lives. I still would say this is a creative use of a model, and probably fine.
    \item \textbf{A hate speech classifier for a social media site}
    \item \textbf{A personal danger classifier for your smartwatch} see instapaper

    \item \textbf{A threat classifier traned on images of war} Wars might change from place to place, if the model was trained in the middle east it might not be as useful in Ukraine, but maybe it could be helpful to augment a security guard looking over thousands of cameras at Grand Central Station. It is not clear to me whether classifying threats falls under creative or critical decision making. Is this a useful classifier for an understaffed security unit, or a bias-reinforcing tool for police abuse? I'm not sure, but I'm sure this model is out there already.\sidenote{if you would like a deeper discussion of ethics please read Reid Blackman's "Ethical Machines"\cite{Blackman2022Jul}, he is pretty good for a Philosophy PhD.}
    \item \textbf{A text generation model claiming to be Artificial General Intelligence}\sidenote{Let's say something like ChatGPT} A model using deep learning techniques, and trained on the entire internet, but claiming to have general purpose answers to everything\sidenote{ChatGPT does not make these claims}. This could absolutely be used for creative endeavors and generate new perspectives but if it is given critical tasks and relied on to "tell the truth", this will probably disappoint. Creating the model isn't a problem in itself, but how it is used or misused might cause a problem.\sidenote{What problems, you ask? Well, give me a few chapters.}
    \item \textbf{A fully self-driving car, one without a steering wheel} The car was trained on millions of miles of road-trials, and maybe a sophisticated simulation environment where it drove trillions of virtual miles.\sidenote{Let's assume the model is only updated once per month here, I'll talk about this in more depth soon.} This model is not giving you a new perspective or funny perspective on driving because all of the training happened in the past. So, as soon as the self-driving model is released, it becomes stale. Any significant changes in the envoronment might cause it to behave strangely and dangerously.
\end{enumerate}
