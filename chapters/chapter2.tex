\setchapterpreamble[u]{\margintoc}
\chapter{The Regression Theory of Everything}
\labch{intro}

\textit{"AI Scientists disagree as to whether these language networks posess true knowledge or are just mimicking humans by remembering the statistics of millions of words. I don't believe any kind of deep learning network will achieve the goal of AGI [Artificial General Intelligence] if the network doesn't model the world the way the brain does. Deep learning networks work well, but not because they solved the knowledge representation problem. They work well because they avoided it completely, relying on statistics and lots of data instead. How deep learning networks work is clever, their performance impressive, and they are commercially valuable. I am only pointing out that they don't possess knowledge and, therefore, are not on the path to having the ability of a five-year-old child."} Jeff Hawkins, 2022 \cite{hawkins_2022}

\section{Let's Avoid Knowledge Representation!}

\section{A Simple Neural Network is also a Linear Regression}

\section{Dummy Variables for Dummies; "It's All Numbers, Man"}

\section{Try That Again With 2,354,356 Parameters}

\section{Multicolinearity and the End of Science}

\section{Let's Test Some Random Inputs! Feature Importance and Explainability}

\section{The Universal Machine Learning Workflow}

Summarize and opine on The Universal Machine Learning Workflow \sidecite{chollet_2022}
