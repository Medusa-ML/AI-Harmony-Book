\let\cleardoublepage\clearpage
\chapter*{Foreword}
\addcontentsline{toc}{chapter}{Foreword} % Add the intro to the table of contents as a chapter

					Foreword


 	Brad Flaugher's book AI Harmony: Blending Human Expertise and AI for Business Success has a clear goal. The author wants people to convince that the new Artificial Intelligence (AI) will be an indispensable tool for the remaining part of the 21st century.

	AI Harmony has two parts. The first four chapters review the transition from classical AI (often called Good Old-Fashioned Artificial Intelligence - GOFAI) to the deep learning models without going into historical and technical details. The second part briefly specifies the scope and limits of two dozen exciting applications, many of which are in the author's close interest.

	AI is the study of how to make computers do things at which people are initially better. AI is now a buzzword, and everybody is interested in how large, creative AI models will transform our lives and labor markets. As we know, there is nothing new under the Sun.

	Alan Turing, in a celebrated paper (Computing Machinery and Intelligence) in 1950, described what is now called "The Turing Test". Turing predicted that in about fifty years, "an average interrogator will not have more than a 70 percent chance of making the right identification after five minutes of questioning".

	Formal AI started with the Dartmouth Conference in 1956. As the organizers claimed, "We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the
conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it?"

	Alan Newell and Herbert Simon, two of the most important pioneers of (GOF)AI and cognitive science, predicted in 1957, "Within ten years a computer will be the world's chess champion unless the rules bar it from competition." Their theory was based on the assumption that "A physical symbol system has the necessary and sufficient means of general intelligent action."

	Initially, AI used purely symbolic methods. Knowledge about the external world and problems are represented by logic and rules. Famously, Logic Theorist was a computer program written in 1956 by Newell, Simon, and Cliff Shaw. The program performed automated reasoning and has been labeled as "the first artificial intelligence program". The initial successes increased expectations and promises, which proved unrealistic, and funding dried, and the first AI Winter arrived. About a decade later the rise of expert systems implied a second boom.

	The origin of the sub-symbolic approach goes back to McCulloch and Pitts, whose model in 1943 wanted to capture the logical structure of the nervous systems. The MCP model (i) introduced a formalism whose refinement and generalization led to the notion of finite automata  (an essential concept in computability theory); (ii) is a technique that inspired the idea of logic design of computers;  (iii)  used first computational algorithms to examine the brain-mind problem, and (iv) offered the first modern computational theory of brain and mind.

	A new era stared with the construction of a Learning Machines. The Perceptron (around 1960) 

is a mathematical construction of an adaptive neural network able to learn and classify inputs. Rosenblatt defined it by adding to the MCP rule a learning rule to modify synaptic weights. Minsky
and Papert proved in 1969 that a single-layer Perceptron could not solve the "exclusive OR" problem, since Perceptrons were assumed to be able to classify only linearly separable patterns. The implication of the critique was the severe restriction on funding neural network research. However, the analysis is not valid for multilayer neural networks. After introducing a new learning algorithm called backpropagation, the field of artificial neural networks became very popular. As it happens with fashions, there were ups and downs. Deep learning algorithms have become very useful in the last ten years. The adjective deep refers to the number of layers the data transforms into. 

The recent big success of large language models elicited deep concerns about whether AI can also learn and execute bad things. The writer of this Foreword is far from being a prophet, so I can only write what we will (not) see …

I found very stimulating the second part of the book, which contain the brief specification of about two dozen case studies. The two most important classes of case studies are  the Classifiers and the Predictors. Prediction is the output of an algorithm trained on a historical dataset and specifically applied to new data to  forecasting the likelihood of a particular outcome. Classifiers are machine learning algorithms that automatically orders or categorizes data into one or more of a set of classes.


Examples for Classifiers on the book are:

Lithium Mining Site Classifier
Large NYSE Stock Order Classifier 
Handwriting Classifier 
Autism Classifier
The Online Ad Server Classifier 
Fake News Classifier
Hate Speech Classifier
Smartwatch Danger Classifier 
 
A number of applications of Predictors:

Models used in finance to predict the likelihood of a company’s shares being purchased through a tender offer.
Horse Racing Prediction
Simple Credit Score  (a type of machine learning model that’s designed to predict an individual’s creditworthiness)
Social Credit Score (a type of machine learning
model that’s designed to predict an individual’s trustworthiness
based on their social behavior and online activities)
The Artificial General Intelligence (AGI) Chatbot: type of machine learning model that’s designed to simulate human-like conversation with users. 

Self-Driving with Statistics and automatic warfare are analyzed in separate chapters. The role of statistical models are emphasized in the future of transportation. The potential of artificial intelligence in the realm of warfare leads very far. Control theory should have a critical role in the development and application of autonomous weaponry.

The model descriptions are very useful. The Reader will learn how the training data were obtained, what are the limits and risks, how expensive might be ts recreation etc. Brad Flaugher convince us that there is no medicine  against progress. Deep learning, even it not a silver bullet, will help us to integrate artificial and human intelligence.

Péter Érdi
Henry Luce Professor of Complex Systems Studies
Kalamazoo College,
Kalamazoo, MI 
                                                              May 2023






