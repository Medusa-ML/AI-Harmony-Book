\let\cleardoublepage\clearpage
\chapter*{Foreword}
\addcontentsline{toc}{chapter}{Foreword} % Add the intro to the table of contents as a chapter

Brad Flaugher's book \textit{AI Harmony: Blending Human Expertise and AI for Business Success} has a clear goal. The author wants people to convince that the new Artificial Intelligence (AI) will be an indispensable tool for the remaining part of the 21st century.

\textit{AI Harmony} has two parts. The first four chapters review the transition from classical AI (often called Good Old-Fashioned Artificial Intelligence - GOFAI) to the deep learning models without going into historical and technical details. The second part briefly specifies the scope and limits of two dozen exciting applications, many of which are in the author's close interest.

AI is the study of how to make computers do things at which people are initially better. AI is now a buzzword, and everybody is interested in how large, creative AI models will transform our lives and labor markets. As we know, there is nothing new under the Sun.

Alan Turing, in a celebrated paper \textit{Computing Machinery and Intelligence} in 1950, described what is now called "The Turing Test". The Turing Test is a challenge to determine if a computer can demonstrate human-like intelligence by having a conversation with a person that's so convincing, the person can't tell they're talking to a machine. Turing predicted that in about fifty years, "an average interrogator will not have more than a 70 percent chance of making the right identification after five minutes of questioning".

Formal AI started with the Dartmouth Conference in 1956. As the organizers claimed, "We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it?"

Alan Newell and Herbert Simon, two of the most important pioneers of (GOF)AI and cognitive science, predicted in 1957, "Within ten years a computer will be the world's chess champion unless the rules bar it from competition." Their theory was based on the assumption that "A physical symbol system has the necessary and sufficient means of general intelligent action."

Initially, AI used purely symbolic methods. Knowledge about the external world and problems are represented by logic and rules. Famously, Logic Theorist was a computer program written in 1956 by Newell, Simon, and Cliff Shaw. The program performed automated reasoning and has been labeled as "the first artificial intelligence program". The initial successes increased expectations and promises, which proved unrealistic, and funding dried, and the first AI Winter arrived. About a decade later the rise of expert systems implied a second boom.

The sub-symbolic approach originated in 1943 with McCulloch and Pitts' model, designed to encapsulate the logical structure of nervous systems. The resultant MCP model established a formalism that, through refinement and generalization, gave rise to finite automata - a critical concept in computability theory. This facilitated logical computer design, deployed the initial computational algorithms for the brain-mind issue, and provided the pioneering computational theory of brain and mind.

A new era stared with the construction of a Learning Machines. The Perceptron (around 1960) is a mathematical construction of an adaptive neural network able to learn and classify inputs. Rosenblatt defined it by adding to the MCP rule a learning rule to modify synaptic weights. Minsky and Papert proved in 1969 that a single-layer Perceptron could not solve the "exclusive OR" problem, since Perceptrons were assumed to be able to classify only linearly separable patterns. The implication of the critique was the severe restriction on funding neural network research. However, the analysis is not valid for multilayer neural networks. After introducing a new learning algorithm called backpropagation, the field of artificial neural networks became very popular. As it happens with fashions, there were ups and downs. Deep learning algorithms have become very useful in the last ten years. The adjective deep refers to the number of layers the data transforms into. 

The considerable achievement of large language models has concurrently raised significant concerns regarding the potential for AI to learn and perform undesirable actions. As the author of this Foreword, my capabilities do not extend to prophesy; hence, I can only offer insights on anticipated developments and what may likely be absent.

I found very stimulating the second part of the book, which contain the brief specification of about two dozen case studies. The two most important classes of case studies are the Classifiers and the Predictors. Prediction is the output of an algorithm trained on a historical dataset and specifically applied to new data to  forecasting the likelihood of a particular outcome. Classifiers are machine learning algorithms that automatically orders or categorizes data into one or more of a set of classes.


Examples for Classifiers on the book are:
\begin{itemize}
\item Lithium Mining Site Classifier
\item Large NYSE Stock Order Classifier 
\item Handwriting Classifier 
\item Autism Classifier
\item The Online Ad Server Classifier 
\item Fake News Classifier
\item Hate Speech Classifier
\item Smartwatch Danger Classifier 
\end{itemize}

A number of applications of Predictors:
\begin{itemize}
\item Models used in finance to predict the likelihood of a companys shares being purchased through a tender offer.
\item Horse Racing Prediction
\item Simple Credit Score  (a type of machine learning model that's designed to predict an individual's creditworthiness)
\item Social Credit Score (a type of machine learning model that’s designed to predict an individual’s trustworthiness
based on their social behavior and online activities)
\item The Artificial General Intelligence (AGI) Chatbot: type of machine learning model that’s designed to simulate human-like conversation with users. 
\end{itemize}

Self-driving cars and autonomous weapons are analyzed in separate chapters. The role of statistical models are emphasized in the future of transportation. The potential of artificial intelligence in the realm of warfare leads very far. Control theory should have a critical role in the development and application of autonomous weaponry.

The model descriptions are very useful. The Reader will learn how the training data were obtained, what are the limits and risks, how expensive might be ts recreation etc. Brad Flaugher convinces us that there is no medicine against progress. Deep learning, even it not a silver bullet, will help us to integrate artificial and human intelligence.

\textit{Kalamazoo, MI} \hfill \textit{Péter Érdi}

\textit {May 2023} \hfill \textit{Henry Luce Professor of Complex Systems Studies}

\hfill \textit{Kalamazoo College} 







